\section{Motivation}
\label{sec:motivation}

We motivate our work by describing the current practice in network configuration and its shortcomings.
In traditional networks with distributed control planes, the operatorsâ€™ goal is to generate the configuration of individual devices. Based on its configuration, the device exchanges information with other (e.g., links costs or best known paths) devices, filters and ranks incoming information, and decides how to forward packets to various destinations.

Device configuration languages are low-level and indirect. For instance, instead of allowing operators to express directly the paths they want through the network, they require operators to specify link costs that result in those paths; instead of allowing operators to express directly the types of traffic to not carry through the network, they require operators to select and program an appropriate filtering mechanism (e.g., route maps, null routing,  access control lists) and instantiate it on topologically appropriate devices; instead of allowing operators to directly specify that they prefer BGP neighbors in a certain order, they require operators to program BGP local preferences and multi-exit discriminators (MED) at each router and ensure that the numbers are consistent across routers.

In many networks today, device configurations are generated manually by operators, without the support of many automated tools. It is easy to see the problems with this approach, such as typos, inconsistency across devices, and no guarantees of policy compliance.

To reduce such problems, some networks use a template-based approach. Configuration templates abstract certain constants into variables (e.g., instead of concrete interface IP address, they may contain a variable {\small \sf{\$$Rtr1\_Int1\_IP$)}} and may use a device vendor-neutral syntax. Operators manually generate the templates, and then use tools to translate them into device configuration, by replacing variables with appropriate constants using a database of network information.
 
 %and replacing vendor-neutral constructs with their vendor-specific counterparts.

%In practice, most networks use a hybrid of templates and manual generation of device configuration. Templates are used for standardized and common configuration elements across devices and the result is manually tweaked to obtain the exact desired network behavior.

While templates avoids some pitfalls of the fully manual approach, they too are far from ideal. The fundamental issue is the semantic mismatch between desirable policies and the level of abstraction that templates offer. While desirable policies are network-wide (e.g., prefer customer networks, or link $A$ should serve as backup for link $B$), templates are device-level (e.g., BGP local preferences or link costs to use). Operators must still manually decompose network-wide policies into device-level policies that can produce the desired network-wide behavior.

This decomposition is not always straightforward and ensuring policy-compliance can be hard, especially in the face of failures. We illustrate this point using two examples based on policies that we have seen in practice. 

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/example1}
\caption{Creating router-level policies is difficult.}
\label{fig:example1}
\end{figure}

Consider the backbone network in Figure~\ref{fig:example1}, with three neighbors, a customer $Cust$ , a peer $Peer$, and a provider $Prov$. The policy of this network is shown on the right. Due to the nature of commercial relationships, it prefers the neighbors in a certain order ($P1$) and does not want to act as a transit between Peer and Prov ($P2$). It prefers to exchange traffic with Cust over $R1$ rather than $R2$ because $R2$ is more expensive or lower-capacity link ($P3$). To guard against another network (known as autonomous system or AS in BGP terminology) "hijacking" prefixes owned by Cust, it only sends traffic to those prefixes if Cust is on the AS path ($P4$). To guard against Cust accidentally becoming a transit for Prov, it does not use Cust for traffic with Prov (and possibly other large networks) in the AS path ($P5$).

To correctly implement this network's policy, the operators must compute and assign router local preferences such that those at Cust-facing interfaces $>$ Peer-facing interfaces $>$ Peer-facing interfaces. At the same time, the preference at $R2$'s Cust-facing interface should be lower than that at $R1$ (but higher than that at Prov-facing interface). To fully realize $P2$, MEDs will have to be appropriately configured as well. To implement $P3$ and $P4$, the operators may assign communities that indicate where a certain routing announcement entered the network. Then, R4 must not announce to Peer routes that have communities that correspond to the R2-prov link but must announce communities for the $R2$-Cust and $R1$-Cust links. Finally, to implement $P4$ and $P5$, the operators will have to compute and configure appropriate prefix-based and AS-path-based filters at each router.

It should be straightforward to see how difficult such policy configuration can be in practice, where the network will have many neighbors across multiple classes of commercial relationships, differing numbers of links per neighbor, along with several region-, neighbor- or prefix-based exceptions to the general policy. Templates only help to certain extent, e.g., keeping preference and community values consistent across routers. The operators must still do much of the work manually, in coming up with the templates themselves. We show later how \sysname enables a direct expression of the policies of such networks and compiles them automatically to individual router configurations. 

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/example2}
\caption{Policy-compliance under failures is difficult.}
\label{fig:example2}
\end{figure}

If configuring network policies is difficult, doing so in a way that ensures policy compliance in the face of failures is even more difficult. Consider the data center network in Figure~\ref{fig:example2} with routers organized as a fat tree and running eBGP with each other.\footnote{To scale and to simplify policy implementation, data center networks increasingly use BGP, with a private ASN per router~\cite{bgp-in-dc-rfc}.} The network has two clusters, one that hosts services that should be reachable globally and one that hosts that should be accessible only internally. This policy is enabled by using non-overlapping address space in the two clusters and ensuring that only the address space for the global services is announced externally. In addition, instead of announcing individual prefixes in this global space, the space is aggregated as one less specific prefix $P_G$.

The operator may decide that a simple way to implement the policy is to have X and Y: $i)$ {\em not export} externally what they hear from routers G and H since these routers border the cluster with local services; and $ii)$ announce what they hear from routers C and D and aggregate to $P_G$ if an announcement is subset of $P_G$. The appeal of this implementation is that X and Y do not need to be made aware of which prefixes are global versus local and IP address assignment can be done independently (e.g., new prefixes can be added to local services without updating router configurations).

However, the implementation above is not correct in the face of failures. Suppose links X--G and X--H fail. Then, X will hear announcements for $P_{l*}$ from C and D, having traversed from G and H to Y to C and D. Per policy implementation, X will start "leaking" these prefixes externally. Depending on the reason to have local services, this leak could impact security (e.g., if the services were sensitive) or availability (e.g., if the $P_{l*}$ prefixes are used for other services outside of the data center). This problem does not happen without failures (and thus in operators' initial testing) because without failures X prefers paths to $P_{l*}$ through G and H since they are shorter. A similar problem will happen if links Y--G and Y--H fail.\footnote{On the other hand, depending on the exact implementation, if links X--C and X--D (or Y--C and Y--D) fail, the network will run into a problem where X (Y) may stop announcing the global prefixes because they would be heard through G and H.}  Link failures in data centers are frequent and it is not uncommon to have multiple failed links at a given time~\cite{dc-failure-study}.

\todo{keep this para only if we discuss aggregation stuff in the design}
To avoid this problem, the operator may decide to disallow paths with "valleys," that is, those that go up, down, and back up again. This can be implemented by $X$ and $Y$ rejecting paths through the other. However, that creates a different problem, aggregation-related blackhole~\cite{xx}, in the face of failures. Suppose links D--A and X--C fail. Now, X will hear announcements for $P_{g2}$ from D and will thus announce the $P_G$ externally. This announcement will bring to X traffic for $P_{g1}$ as well, but because of valley-free filtering, X does not have a valid route for $P_{g1}$ and will thus drop all traffic to it.

Given high-level policy specification, \sysname implements them in a way that guarantees compliance under all failures if that is possible. Otherwise, it generates a compiler error that informs the operator that their specification cannot be met. For features like aggregation, it will also provide a lower bound to operators on the number of failures under which aggregation will not result in blackholes.


%\subsection{OLD -- Overview}
%\todo{this subsection can be deleted once we have captured everything in it}
%
%Sources of bugs we fix. (It seems like the main advantage of our approach is not just fixing bugs though - it is easily describing high-level intention).
%Perhaps the best way to do this is by going through a bunch of examples in section 2 and showing how you could easily introduce bugs:
%
%\begin{itemize}
%	\item Out of sync, or copy paste errors due to replicated configs (never an issue due to centralized control)
%	\item Correct filtering to ensure no undesired traffic can flow through the network (e.g., best practices, like an AS should filter customers for their prefix, are implemented automatically )
%	\item Failures can easily lead to unexpected behavior (e.g., a datacenter failure scenario w/instability.)
%	\item Trying to do anything interesting, like get backups correct is difficult (e.g., setting up aggregation wrong)
%	\item Related to the last point, things like aggregation can introduce black holes. We have the information needed to prevent this
%\end{itemize}
%
%
%Possible Examples:
%\begin{itemize}
%	\item Basic datacenter with spine preference
%	\item Simple AS that prefers customers over peers over providers
%	\item Combined internal backup routing with preference based entrance into the network using aggregation.
%	\item Something like cold potato routing
%\end{itemize}

