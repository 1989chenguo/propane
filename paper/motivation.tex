\section{Motivation}
\label{sec:motivation}

%We motivate our work by describing the current practice in network
%configuration. In traditional networks with distributed control planes, the operators' goal is to generate the configuration of individual devices based on their desired policy. This configuration dictates the behavior of a device, how it exchanges routing information with neighbors and how it filters and ranks that information. The collective behavior of the devices should implement the desired policy.
%
%Device configuration languages are low-level and indirect. For instance, instead of allowing operators to express directly the paths they want through the network, they require operators to specify metrics that result in those paths; instead of allowing operators to express directly the types of traffic to not carry through the network, they require operators to select and program an appropriate filtering mechanism (e.g., BGP import or export filters, null routing,  access control lists) and instantiate it on topologically appropriate devices; instead of allowing operators to directly specify that they prefer BGP neighbors in a certain order, they require operators to program local preferences and multi-exit discriminators (MED) at each router and ensure that the numbers are consistent across routers.
%
%In many networks today, device configurations are generated manually by operators, without the support of many automated tools. It is easy to see the problems with this approach, such as typos, inconsistency across devices, and no guarantees of policy compliance.
%
%To reduce such problems, some networks use a template-based approach. Configuration templates abstract certain constants into variables (e.g., instead of concrete community value, they may contain a variable {\small \sf{\$$BadNeighbor$)}} and may use a device vendor-neutral syntax. Operators manually generate the templates and use tools to translate them into device configuration, by replacing variables with appropriate constants using a database of network information.
%
% %and replacing vendor-neutral constructs with their vendor-specific counterparts.
%
%%In practice, most networks use a hybrid of templates and manual generation of device configuration. Templates are used for standardized and common configuration elements across devices and the result is manually tweaked to obtain the exact desired network behavior.
%
%While templates avoids some pitfalls of the fully manual approach, they too are far from ideal. The fundamental issue is the semantic mismatch between desired policies and the level of abstraction of templates. While many policies are network-wide (e.g., prefer customer networks, or never announce a route to a certain prefix to external neighbors), templates are device-level. Operators must still manually decompose network-wide policies into device-level policies that can produce the desired network-wide behavior.
%This decomposition is not always straightforward and ensuring policy-compliance can be hard, especially in the face of failures. We illustrate this point using two examples based on policies that we have seen in practice and we demonstrate the ease with which the
%examples are described using our new language \sysname.

Operators today generate the BGP configuration of individual devices based on intended policy. Whether this task is done fully manually or with the aid of templates, the challenge is to decompose network-wide policies
%(e.g., prefer customer networks, or never announce a route to a certain prefix to external neighbors),
into device-level policies that collectively produce the intended behavior.
This decomposition is not always straightforward and ensuring policy-compliance can be hard, especially in the face of failures. We illustrate this point using two examples based on policies that we have seen in practice.
%In the next section, we demonstrate the ease with which the
%policies can be specified using our new language \sysname.
%The next section describes how to automatically compile this specification to device-level policies.

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/example1}
\caption{Creating router-level policies is difficult.}
\label{fig:example1}
\end{figure}

\subsection{Example 1:  The Backbone}

Consider the backbone network in Figure~\ref{fig:example1}. It has three neighbors, a customer $Cust$ , a peer $Peer$, and a provider $Prov$. The policy of this network is shown on the right. It prefers the neighbors in a certain order ($P1$) and does not want to act as a transit between Peer and Prov ($P2$). It prefers to exchange traffic with Cust over $R1$ rather than $R2$ because $R1$ is a cheaper ($P3$). To guard against another AS "hijacking" prefixes owned by Cust, it only sends traffic to them if Cust is on the AS path ($P4$). Finally, to guard against Cust accidentally becoming a transit for Prov, it does not use Cust for traffic that will later traverse Prov ($P5$).

To correctly implement this policy, the operators must compute and assign local preferences such that preferences at Cust-facing interfaces $>$ Peer-facing interfaces $>$ Prov-facing interfaces. At the same time, the preference at $R2$'s Cust-facing interface should be lower than that at $R1$, but higher than that at Prov-facing interface. To fully realize $P2$, MEDs will have to be appropriately configured as well. To implement $P3$ and $P4$, the operators may assign communities that indicate where a certain routing announcement entered the network. Then, R4 must not announce to $Peer$ routes that have communities that correspond to the R2-prov link but must announce communities for the $R2$-Cust and $R1$-Cust links. Finally, to implement $P4$ and $P5$, the operators will have to compute and configure appropriate prefix- and AS-path-based filters at each router.

We can now see how devising correct configurations for real, larger networks can be a nightmare. Such networks have many neighbors across multiple commercial-relationship classes, differing numbers of links per neighbor, along with several geography-, neighbor- or prefix-based exceptions to the default behavior. Templates help by keeping preference and community values consistent across routers, but operators must still do much of the conceptually difficult work manually.
\sysname lets operator express such policies easily and intuitively. It them automatically generates per-device import and export filters, local preferences, MED attributes, and community tags to ensure that the policy is implemented correctly under all failure scenarios.


\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/example2}
\caption{Policy-compliance under failures is difficult.}
\label{fig:example2}
\end{figure}

\subsection{Example 2:  The Data Center}

 While configuring policies for a fully functional network is difficult, ensuring policy compliance in the face of failures can be almost impossible. Consider the data center network in Figure~\ref{fig:example2} with routers organized as a fat tree and running BGP.\footnote{For scale and policy flexibility, data center networks increasingly use BGP internally, with a private AS number per router~\cite{bgp-in-dc-rfc}.} The network has two clusters, one with services that should be reachable globally and one with services that should be accessible only internally. This policy is enabled by using non-overlapping address space in the two clusters and ensuring that only the address space for the global services is announced externally. Further, to reduce the number of prefixes that are announced externally, the global space is aggregated into a less-specific prefix $P_G$. The semantics of aggregation is that the aggregate prefix is announced as long as the router has a path to at least one sub-prefix.

The operator may decide that a simple way to implement the policy is to have X and Y: $i)$ not export externally what they hear from G and H, routers that belong to the local services cluster; and $ii)$ announce what they hear from routers C and D and aggregate to $P_G$ if an announcement is subset of $P_G$. This implementation is appealing because X and Y do not need to be made aware of which prefixes are global versus local and IP address assignment can be done independently (e.g., new prefixes can be assigned to local services without updating router configurations).

However, this implementation does not have the right behavior in the face of failures. Suppose links X--G and X--H fail. Then, X will hear announcements for $P_{l*}$ from C and D, having traversed from G and H to Y to C and D. Per policy implementation, X will start "leaking" these prefixes externally. Depending on the rationale for local services, this leak could impact security (e.g., if the services were sensitive) or availability (e.g., if the $P_{l*}$ prefixes are used for other services outside of the data center). This problem does not manifest without failures because then X has and prefers paths to $P_{l*}$ through G and H since they are shorter. A similar problem will happen if links Y--G and Y--H fail.
%\footnote{A different problem occurs when links X--C and X--D (or Y--C and Y--D) fail. X (Y) may stop announcing the global prefixes because they would be heard through G and H.}
Link failures in data centers are frequent and it is not uncommon to have multiple failed links at a given time~\cite{dc-failure-study}.

To avoid this problem, the operator may decide to disallow "valleys" paths, i.e., those that go up, down, and back up again. This guard can be implemented by $X$ and $Y$ rejecting paths through the other. But that creates a different problem in the face of failures---an aggregation-induced blackhole~\cite{xx}. If links D--A and X--C fail, X will hear announcements for $P_{g2}$ from D and will thus announce the $P_G$ externally. This announcement will bring to X traffic for $P_{g1}$ as well, but because of valley-free filtering, X does not have a valid route for $P_{g1}$ and will thus drop all traffic to it.

Thus, we see that devising a configuration that ensures policy compliance in the face of failures is complex and error-prone. \sysname lets operators implement their high-level policy specification in a way that guarantees compliance under all failures (if that is possible; otherwise, it generates a compile-time error). For aggregation, it will also provide a lower bound to operators on the number of failures under which aggregation will not result in blackholes.



%\subsection{OLD -- Overview}
%\todo{this subsection can be deleted once we have captured everything in it}
%
%Sources of bugs we fix. (It seems like the main advantage of our approach is not just fixing bugs though - it is easily describing high-level intention).
%Perhaps the best way to do this is by going through a bunch of examples in section 2 and showing how you could easily introduce bugs:
%
%\begin{itemize}
%	\item Out of sync, or copy paste errors due to replicated configs (never an issue due to centralized control)
%	\item Correct filtering to ensure no undesired traffic can flow through the network (e.g., best practices, like an AS should filter customers for their prefix, are implemented automatically )
%	\item Failures can easily lead to unexpected behavior (e.g., a datacenter failure scenario w/instability.)
%	\item Trying to do anything interesting, like get backups correct is difficult (e.g., setting up aggregation wrong)
%	\item Related to the last point, things like aggregation can introduce black holes. We have the information needed to prevent this
%\end{itemize}
%
%
%Possible Examples:
%\begin{itemize}
%	\item Basic datacenter with spine preference
%	\item Simple AS that prefers customers over peers over providers
%	\item Combined internal backup routing with preference based entrance into the network using aggregation.
%	\item Something like cold potato routing
%\end{itemize}

